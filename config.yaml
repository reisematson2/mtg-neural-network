# Default configuration for training and evaluation

paths:
  data: card_data.csv
  checkpoint_dir: checkpoints

training:
  epochs: 10 # maximum number of epochs
  batch_size: 16
  learning_rate: 0.001
  weight_decay: 0.00001 # L2 regularization in optimizer
  scheduler:
    type: "step" # options: "step", "cosine"
    step_size: 5 # for step scheduler, reduce LR every N epochs
    gamma: 0.1 # LR multiplier at each step
  early_stopping:
    patience: 3 # stop if val_loss doesnâ€™t improve in this many epochs
  seed: 42
  log_interval: 10

model:
  embed_dim: 128
  lstm_dim: 32
  hidden_dim: 64
  dropout_rate: 0.3 # increase from 0.2 for stronger regularization
